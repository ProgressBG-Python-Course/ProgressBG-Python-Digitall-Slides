{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Integration Pipeline Project\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "The goal of this project is to demonstrate a simple yet effective data integration pipeline that extracts data from various sources, transforms it, and loads it into a database. This pipeline is designed to simulate a real-world scenario where a company needs to integrate marketing data from Facebook Ads and website interaction data from Google Analytics into their Customer Relationship Management (CRM) system.\n",
    "\n",
    "This process involves extracting raw data from CSV files (representing Facebook Ads data) and JSON files (representing Google Analytics data), transforming this data to ensure quality and consistency, and then loading the cleaned data into an SQLite database using SQLAlchemy. \n",
    "\n",
    "The project showcases how to handle different data formats and structures, perform data cleaning and transformation operations, and use Python and SQLAlchemy for database interactions.\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "```plaintext\n",
    "data-integration-demo/\n",
    "│\n",
    "├── data/\n",
    "│   ├── facebook_ads_data.csv  # Facebook Ads data in CSV format\n",
    "│   └── google_analytics_data.json  # Google Analytics data in JSON format\n",
    "│\n",
    "├── src/\n",
    "│   ├── __init__.py  # Marks src/ as a Python package\n",
    "│   ├── actions.py  # Contains functions for database operations\n",
    "│   ├── extract.py  # Functions for extracting data from CSV and JSON files\n",
    "│   ├── load.py  # Functions that load transformed data into the database\n",
    "│   ├── models.py  # SQLAlchemy ORM models defining the database schema\n",
    "│   └── transform.py  # Functions for data transformation and cleaning\n",
    "│\n",
    "└── main.py  # The main script that orchestrates the ETL process\n",
    "```\n",
    "\n",
    "\n",
    "### data\n",
    "Here, we store our raw data files, such as CSV files for Facebook Ads data (`facebook_ads_data.csv`) and JSON files for Google Analytics data (`google_analytics_data.json`). Having a dedicated directory for data helps keep our project organized and simplifies data management.\n",
    "\n",
    "### main.py\n",
    "This is the entry point of our project. It orchestrates the data integration process by initializing the database with the models defined in `models.py`, extracting data using functions in `extract.py`, transforming this data with `transform.py`, and finally, loading the processed data into the database with `load.py`. This structure allows for modular development and easy updates or expansions.\n",
    "\n",
    "### src\n",
    "A package that contains the core functionality of our data integration pipeline, organized into modules based on their roles.\n",
    "\n",
    "#### src/actions.py\n",
    "This module contains functions for direct database interactions, such as inserting or updating records. These are usually called from `load.py` to persist the cleaned and transformed data.\n",
    "\n",
    "#### src/extract.py\n",
    "Focused on extracting data, this module has functions to read and load data from our raw sources. Organizing our extraction logic here makes it easier to adapt as our data sources evolve.\n",
    "\n",
    "#### src/__init__.py\n",
    "A file that marks `src` as a Python package, enabling the importation of its modules (`models`, `actions`, `extract`, `transform`, `load`) within `main.py` or amongst themselves.\n",
    "\n",
    "#### src/load.py\n",
    "Contains logic to load the transformed data into the database, typically utilizing functions from `actions.py` for database insertion or updates. This separation facilitates clean, maintainable code.\n",
    "\n",
    "#### src/models.py\n",
    "Defines our database schema using SQLAlchemy ORM models, where each class represents a database table. This abstraction simplifies database operations.\n",
    "\n",
    "#### src/transform.py\n",
    "Here, we focus on data transformation and cleaning. The raw data extracted by `extract.py` is processed to clean, normalize, or aggregate as needed before being ready for database insertion by `load.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Input Data\n",
    "\n",
    "Our Data Integration Pipeline Project leverages two primary sources of marketing data: Facebook Ads and Google Analytics. Understanding the structure, content, and significance of data from these platforms is crucial for effective extraction, transformation, and integration.\n",
    "\n",
    "#### Facebook Ads Data\n",
    "\n",
    "**Source**: CSV file (`facebook_ads_data.csv`)\n",
    "\n",
    "**Description**: This dataset contains information about advertising campaigns run on Facebook. It's structured in a tabular format with the following key columns:\n",
    "\n",
    "- `campaign_id`: A unique identifier for each ad campaign.\n",
    "- `campaign_name`: The name of the ad campaign.\n",
    "- `date`: The date on which the ad was shown.\n",
    "- `impressions`: The number of times the ad was displayed to users.\n",
    "- `clicks`: The number of times users clicked on the ad.\n",
    "- `spend`: The total amount of money spent on the ad campaign for the given date.\n",
    "\n",
    "**Purpose**: The Facebook Ads data provides insights into the reach and effectiveness of marketing efforts on the Facebook platform. Analyzing this data helps understand how different campaigns perform in terms of user engagement and cost-efficiency.\n",
    "\n",
    "#### Google Analytics Data\n",
    "\n",
    "**Source**: JSON file (`google_analytics_data.json`)\n",
    "\n",
    "**Description**: This dataset captures user interactions on a website, as tracked by Google Analytics. The data is structured in a nested JSON format, containing records with these key fields:\n",
    "\n",
    "- `campaign_id`: A unique identifier that can link website interactions back to specific ad campaigns, correlating with the `campaign_id` in the Facebook Ads data.\n",
    "- `date`: The date of the website interactions.\n",
    "- `metrics`: A nested object containing several metrics:\n",
    "  - `page_views`: The total number of page views on the website.\n",
    "  - `conversions`: The number of conversions or goal completions on the website.\n",
    "  - `bounce_rate`: The percentage of single-page sessions in which there was no interaction with the page.\n",
    "\n",
    "**Purpose**: Google Analytics data offers a window into how users interact with a website following engagement with marketing campaigns. It allows for the assessment of website performance and the effectiveness of marketing strategies in driving desired user actions.\n",
    "\n",
    "#### Importance of Integration\n",
    "\n",
    "By integrating Facebook Ads and Google Analytics data, we aim to construct a comprehensive view of our marketing funnel, from ad impressions through to website engagements and conversions. This integrated perspective is invaluable for optimizing marketing strategies, allocating budgets more effectively, and ultimately enhancing ROI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract\n",
    "\n",
    "The Extract phase is the first step in our data integration pipeline. During this phase, we gather data from various sources, which, in this project, include CSV files for Facebook Ads data and JSON files for Google Analytics data. This step is crucial for preparing the data for further processing and analysis.\n",
    "\n",
    "To handle the extraction process, we've dedicated a module named `extract.py` within the `src/` directory of our project structure. This module contains functions for reading data from CSV and JSON files, effectively loading this data into Python data structures for the subsequent Transform phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Load facebook ads data\n",
    "\n",
    "Facebook Ads data is stored in CSV format. We will create a `load_facebook_ads_data()` function which will use Pandas to read this data. \n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "def load_facebook_ads_data(csv_file_path):\n",
    "    \"\"\"Loads Facebook Ads data from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    csv_file_path (str): The path to the CSV file containing Facebook Ads data.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the loaded data.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(csv_file_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Load google analytics data\n",
    "\n",
    "Google Analytics data is stored in a nested JSON format. We also use Pandas to read this data, taking advantage of its read_json function to directly load JSON data into a DataFrame.\n",
    "\n",
    "```python\n",
    "def load_google_analytics_data(json_file_path):\n",
    "\"\"\"Loads Google Analytics data from a JSON file.\n",
    "\n",
    "Parameters:\n",
    "json_file_path (str): The path to the JSON file containing Google Analytics data.\n",
    "\n",
    "Returns:\n",
    "pandas.DataFrame: A DataFrame containing the loaded data.\n",
    "\"\"\"\n",
    "return pd.read_json(json_file_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "\n",
    "Following the successful extraction of Facebook Ads data and Google Analytics data, the next critical step in our Data Integration Pipeline Project is the Transform phase. This phase is where the raw data, extracted in various formats, undergoes cleaning, normalization, and preparation to ensure it's ready for loading into the CRM system. The transformation process is key to resolving discrepancies between different data sources, enhancing data quality, and ensuring the data is in a usable format for analysis and integration.\n",
    "\n",
    "#### Objectives of the Transform Phase:\n",
    "\n",
    "- **Data Cleaning**: Addressing issues such as missing values, duplicate records, and incorrect data entries that may affect the quality and reliability of the data.\n",
    "- **Normalization**: Ensuring consistency across datasets, particularly in terms of formats for dates, numeric fields, and categorical data, to facilitate accurate comparison and analysis.\n",
    "- **Data Enrichment**: Enhancing the data with additional computations or derivations, such as calculating the click-through rate (CTR) for Facebook Ads or conversion rates for Google Analytics sessions.\n",
    "- **Data Structuring**: Transforming nested JSON structures into a flat, tabular format that aligns with the relational structure of the CRM database, making it easier to load and query the data.\n",
    "\n",
    "We will implement the respective transformation functions into `src/transform.py` module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load\n",
    "\n",
    "We'll define functions in load.py that orchestrate the loading of cleaned and transformed data into the CRM system. These functions will utilize the actions.py module for database operations, focusing on integrating Facebook Ads and Google Analytics data.\n",
    "\n",
    "Another option is to load our integrated data to API Server using json format."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
